# Retail Data Engineering Project Documentation
# ğŸ“Œ Project Overview

This project implements an **end-to-end retail data pipeline** using Azure Data Engineering tools. The pipeline ingests data from multiple sources, transforms it into a structured data lake format (Bronze â†’ Silver â†’ Gold layers), and enables business reporting through Power BI.

---

## Tech Stack:

- **Azure Data Factory (ADF)** â†’ Data ingestion

- **Azure Data Lake Storage (ADLS)** â†’ Central data lake (Bronze/Silver/Gold)

- **Azure Databricks** â†’ Data cleaning, transformation, aggregation

- **Power BI** â†’ Reporting & dashboards

## ğŸ“‚ Data Sources

**1. Azure SQL Database**

 - **Transactions** (sales data)

 - **Stores** (store details)

- **Products** (catalog details)

**2. API / JSON**

- **Customers** (customer master data in JSON format)

---

## ğŸ—ï¸ Architecture
```
graph LR
A[Azure SQL - Transactions] --> B[ADF]
C[Azure SQL - Stores] --> B
D[Azure SQL - Products] --> B
E[API - Customers JSON] --> B
B --> F[ADLS - Bronze]
F --> G[Databricks - Silver Layer]
G --> H[Databricks - Gold Layer]
H --> I[Power BI Reports]
```
---

- **Bronze Layer â†’** Raw ingestion from SQL & JSON

- **Silver Layer â†’** Cleaned, standardized, joined dataset

- **Gold Layer â†’** Aggregated business-level KPIs for reporting

## âš™ï¸ Data Processing
**Bronze Layer**

- Raw parquet files from ADF ingestion

- Stores, Products, Transactions, Customers as-is

**Silver Layer**

- Cleaned using PySpark (```retail projects - multiple tables (1).py```):

- Schema alignment (casting datatypes)

- Removing duplicates

- Joining customers, stores, products, transactions

- Adding derived column: ```total_amount = quantity * price```

**Gold Layer**

- Aggregated metrics created:

- total_quantity_sold

- total_sales_amount

- number_of_transactions

- average_transaction_value

Stored as **Delta tables** for optimized analytics.

---

## ğŸ“Š Business Requirements & KPIs
**1. Total Sales by Store and Category**

- **Metric:** SUM(total_sales_amount)

- **Dimensions:** Store, Category

- **Visualization:** Grouped Bar Chart

**2. Daily Sales Trend by Product**

 -**Metric:** SUM(total_sales_amount)

 - **Dimensions:** Date, Product

 - **Visualization:** Line Chart (time series)

**3. Average Order Value per Store**

 - **Metric:** ```SUM(total_sales_amount) / COUNT(transaction_id)```

 - **Visualization:** Column Chart (per store)

**4. Heatmap: Store vs Sales**

- **Metric:** SUM(total_sales_amount)

- **Dimensions:** Store Ã— Product/Category

- **Visualization:** Heatmap (intensity by sales)

---  

## ğŸ“‘ Power BI Reporting

- Connect Power BI to **Gold Layer Delta tables**

- Create dashboard with:

  - ğŸ“Š Sales by Store & Category

  - ğŸ“ˆ Daily Sales Trend by Product

  - ğŸ’° AOV per Store

  - ğŸ”¥ Heatmap: Store vs Sales

## ğŸš€ Deployment Steps

**1.** Deploy SQL schema & sample data (```SCRIPT SQL.txt```)

**2.** Upload ```customers.json``` to API/Blob for ingestion

**3.** Configure ADF pipelines:

 - Copy data from SQL & JSON to ADLS Bronze

**4.** Run Databricks notebook (```retail projects - multiple tables (1).py```)

 - Create Silver & Gold tables

**5.** Connect Power BI â†’ Gold Layer Delta tables

Build dashboards as per KPIs

---

## ğŸ“¦ Repository Structure

```

â”œâ”€â”€ customers.json                  # Customer data (JSON)
â”œâ”€â”€ SCRIPT SQL.txt                  # SQL schema + inserts
â”œâ”€â”€ retail projects - multiple tables (1).py   # Databricks ETL script
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.png            # Architecture diagram
â”‚   â””â”€â”€ dashboard_wireframes.png    # Power BI mockups
â””â”€â”€ README.md                       # Project documentation

```

---

## ğŸ“Œ Future Enhancements

- Automate pipeline scheduling with ADF triggers

- Add incremental data loads (CDC for transactions)

- Implement role-based security in Power BI

- Integrate ML models for demand forecasting

- Author: Retail Data Engineering Team
---
